{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112499bb",
   "metadata": {
    "papermill": {
     "duration": 0.018739,
     "end_time": "2022-04-11T10:00:40.393442",
     "exception": false,
     "start_time": "2022-04-11T10:00:40.374703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Classification: \n",
    "Classification is to identify which category a new observation belongs to, on the basis of a training dataset. There are five datasets. For each dataset, we provide the training dataset, training label, and test dataset. Please use the training dataset and training label to build your classifier and predict the test label. A class label is represented by an integer. For example, in the 1st dataset, there are 4 classes where 1 represents the 1st class, 2 represents the 2nd class, etc. Note that, there exist some missing values in some of the dataset (a missing entry is filled by 1.00000000000000e+99), please fill the missing values before perform your classification algorithm.\n",
    "\n",
    "TrainData 1 contains 3312 features with 150 samples. Testdata1 contains 3312 features with 53 samples. There are 5 classes in this dataset.\n",
    "\n",
    "TrainData 2 contains 9182 features with 100 samples. Testdata2 contains 9182 features with 74 samples. There are 11 classes in this dataset.\n",
    "\n",
    "TrainData 3 contains 13  features with 6300 samples. Testdata3 contains 13 features with 2693 samples. There are 9 classes in this dataset.\n",
    "\n",
    "TrainData 4 contains 112 features with 2547 samples. Testdata4 contains 112 features with 1092 samples. There are 9 classes in this dataset.\n",
    "\n",
    "TrainData 5 contains 11 features with 1119 samples. Testdata5 contains 11 features with 480 samples. There are 6 classes in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e74fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:00:40.442318Z",
     "iopub.status.busy": "2022-04-11T10:00:40.441644Z",
     "iopub.status.idle": "2022-04-11T10:00:40.456342Z",
     "shell.execute_reply": "2022-04-11T10:00:40.455601Z",
     "shell.execute_reply.started": "2022-04-11T09:40:49.765866Z"
    },
    "papermill": {
     "duration": 0.04511,
     "end_time": "2022-04-11T10:00:40.456500",
     "exception": false,
     "start_time": "2022-04-11T10:00:40.411390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/test-data-1/MultLabelTrainData.txt\n",
      "/kaggle/input/test-data-1/TrainData5.txt\n",
      "/kaggle/input/test-data-1/TrainData4.txt\n",
      "/kaggle/input/test-data-1/TrainLabel3.txt\n",
      "/kaggle/input/test-data-1/TestData4.txt\n",
      "/kaggle/input/test-data-1/TrainLabel4.txt\n",
      "/kaggle/input/test-data-1/MissingData2.txt\n",
      "/kaggle/input/test-data-1/TestData6.txt\n",
      "/kaggle/input/test-data-1/TrainData3.txt\n",
      "/kaggle/input/test-data-1/MissingData1.txt\n",
      "/kaggle/input/test-data-1/TrainData6.txt\n",
      "/kaggle/input/test-data-1/TrainData1.txt\n",
      "/kaggle/input/test-data-1/TestData5.txt\n",
      "/kaggle/input/test-data-1/TestData3.txt\n",
      "/kaggle/input/test-data-1/TestData1.txt\n",
      "/kaggle/input/test-data-1/TrainLabel6.txt\n",
      "/kaggle/input/test-data-1/MissingData3.txt\n",
      "/kaggle/input/test-data-1/MultLabelTestData.txt\n",
      "/kaggle/input/test-data-1/TestData2.txt\n",
      "/kaggle/input/test-data-1/TrainLabel5.txt\n",
      "/kaggle/input/test-data-1/TrainData2.txt\n",
      "/kaggle/input/test-data-1/TrainLabel1.txt\n",
      "/kaggle/input/test-data-1/MultLabelTrainLabel.txt\n",
      "/kaggle/input/test-data-1/TrainLabel2.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77021405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:00:40.500608Z",
     "iopub.status.busy": "2022-04-11T10:00:40.499753Z",
     "iopub.status.idle": "2022-04-11T10:00:41.791367Z",
     "shell.execute_reply": "2022-04-11T10:00:41.790595Z",
     "shell.execute_reply.started": "2022-04-11T09:40:49.845257Z"
    },
    "papermill": {
     "duration": 1.316283,
     "end_time": "2022-04-11T10:00:41.791518",
     "exception": false,
     "start_time": "2022-04-11T10:00:40.475235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25383fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:00:41.840905Z",
     "iopub.status.busy": "2022-04-11T10:00:41.840121Z",
     "iopub.status.idle": "2022-04-11T10:00:44.624347Z",
     "shell.execute_reply": "2022-04-11T10:00:44.623694Z",
     "shell.execute_reply.started": "2022-04-11T09:40:49.974125Z"
    },
    "papermill": {
     "duration": 2.814838,
     "end_time": "2022-04-11T10:00:44.624505",
     "exception": false,
     "start_time": "2022-04-11T10:00:41.809667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "traindata1 = pd.read_csv('/kaggle/input/test-data-1/TrainData1.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "traindata2 = pd.read_csv('/kaggle/input/test-data-1/TrainData2.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "traindata3 = pd.read_csv('/kaggle/input/test-data-1/TrainData3.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "traindata4 = pd.read_csv('/kaggle/input/test-data-1/TrainData4.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "traindata5 = pd.read_csv('/kaggle/input/test-data-1/TrainData5.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "train_data = [traindata1, traindata2, traindata3, traindata4, traindata5]\n",
    "\n",
    "testdata1 = pd.read_csv('/kaggle/input/test-data-1/TestData1.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "testdata2 = pd.read_csv('/kaggle/input/test-data-1/TestData2.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "testdata3 = pd.read_csv('/kaggle/input/test-data-1/TestData3.txt', sep=',', header=None, na_values=1000000000)\n",
    "testdata4 = pd.read_csv('/kaggle/input/test-data-1/TestData4.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "testdata5 = pd.read_csv('/kaggle/input/test-data-1/TestData5.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "test_data = [testdata1, testdata2, testdata3, testdata4, testdata5]\n",
    "\n",
    "trainlabel1 = pd.read_csv('/kaggle/input/test-data-1/TrainLabel1.txt', sep='\\t', header=None)\n",
    "trainlabel2 = pd.read_csv('/kaggle/input/test-data-1/TrainLabel2.txt', sep='\\t', header=None)\n",
    "trainlabel3 = pd.read_csv('/kaggle/input/test-data-1/TrainLabel3.txt', sep='\\t', header=None)\n",
    "trainlabel4 = pd.read_csv('/kaggle/input/test-data-1/TrainLabel4.txt', sep='\\t', header=None)\n",
    "trainlabel5 = pd.read_csv('/kaggle/input/test-data-1/TrainLabel5.txt', sep='\\t', header=None)\n",
    "train_label = [trainlabel1, trainlabel2, trainlabel3, trainlabel4, trainlabel5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051fe31f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:00:44.668565Z",
     "iopub.status.busy": "2022-04-11T10:00:44.667866Z",
     "iopub.status.idle": "2022-04-11T10:00:44.765236Z",
     "shell.execute_reply": "2022-04-11T10:00:44.764671Z",
     "shell.execute_reply.started": "2022-04-11T09:40:52.417301Z"
    },
    "papermill": {
     "duration": 0.121965,
     "end_time": "2022-04-11T10:00:44.765389",
     "exception": false,
     "start_time": "2022-04-11T10:00:44.643424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the train data:\n",
      "train dataset 1: 9936\n",
      "train dataset 2: 0\n",
      "train dataset 3: 1886\n",
      "train dataset 4: 0\n",
      "train dataset 5: 0\n",
      "Missing values in the test data:\n",
      "test dataset 1: 7021\n",
      "test dataset 2: 0\n",
      "test dataset 3: 808\n",
      "test dataset 4: 0\n",
      "test dataset 5: 0\n"
     ]
    }
   ],
   "source": [
    "# count the numbers of coloumns where the value = 1.00000000000000e+99\n",
    "def count_na(data, type):\n",
    "    print(\"Missing values in the \" + type + \" data:\")\n",
    "    for i in range(len(data)):\n",
    "        missing_val_count = data[i].isnull().sum().sum()\n",
    "        print( type + \" dataset \" + str(i+1) + \": \" + str(missing_val_count))\n",
    "\n",
    "count_na(train_data, \"train\")\n",
    "count_na(test_data, \"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c00f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:00:44.855807Z",
     "iopub.status.busy": "2022-04-11T10:00:44.854705Z",
     "iopub.status.idle": "2022-04-11T10:00:49.007517Z",
     "shell.execute_reply": "2022-04-11T10:00:49.006824Z",
     "shell.execute_reply.started": "2022-04-11T09:40:52.521351Z"
    },
    "papermill": {
     "duration": 4.223428,
     "end_time": "2022-04-11T10:00:49.007659",
     "exception": false,
     "start_time": "2022-04-11T10:00:44.784231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the train data:\n",
      "train dataset 1: 0\n",
      "train dataset 2: 0\n",
      "train dataset 3: 0\n",
      "train dataset 4: 0\n",
      "train dataset 5: 0\n",
      "Missing values in the test data:\n",
      "test dataset 1: 0\n",
      "test dataset 2: 0\n",
      "test dataset 3: 808\n",
      "test dataset 4: 0\n",
      "test dataset 5: 0\n"
     ]
    }
   ],
   "source": [
    "# Now we need to replace the missing values in the dataset using the mean of each column\n",
    "def replace_na(data):\n",
    "    for i in range(len(data)):\n",
    "        for column in data[i].columns:   \n",
    "                data[i][column] = data[i][column].fillna(data[i][column].mean())\n",
    "         \n",
    "replace_na(train_data)\n",
    "\n",
    "# Replacing test dataset 1 missing values\n",
    "for column in testdata1.columns:\n",
    "    testdata1[column] = testdata1[column].fillna(testdata1[column].mean())\n",
    "\n",
    "# Now checking if any remaining missing vals left\n",
    "count_na(train_data, \"train\")\n",
    "count_na(test_data, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf88437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:00:49.084289Z",
     "iopub.status.busy": "2022-04-11T10:00:49.083075Z",
     "iopub.status.idle": "2022-04-11T10:00:49.310904Z",
     "shell.execute_reply": "2022-04-11T10:00:49.310341Z",
     "shell.execute_reply.started": "2022-04-11T09:40:56.956925Z"
    },
    "papermill": {
     "duration": 0.284236,
     "end_time": "2022-04-11T10:00:49.311085",
     "exception": false,
     "start_time": "2022-04-11T10:00:49.026849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For all the train data, remove the columns with low variance\n",
    "def remove_low_variance(data):\n",
    "    for i in range(len(data)):\n",
    "        selector = VarianceThreshold(threshold=.8 * (1 - .8))\n",
    "        data[i] = selector.fit_transform(data[i])\n",
    "\n",
    "remove_low_variance(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535d9476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:00:49.360284Z",
     "iopub.status.busy": "2022-04-11T10:00:49.359583Z",
     "iopub.status.idle": "2022-04-11T10:01:41.243911Z",
     "shell.execute_reply": "2022-04-11T10:01:41.243263Z",
     "shell.execute_reply.started": "2022-04-11T09:40:57.223219Z"
    },
    "papermill": {
     "duration": 51.914207,
     "end_time": "2022-04-11T10:01:41.244085",
     "exception": false,
     "start_time": "2022-04-11T10:00:49.329878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove correlated features\n",
    "def remove_correlation(data, threshold):\n",
    "    for i in range(len(data)):\n",
    "        data[i] = pd.DataFrame(data[i])\n",
    "        col_corr = set()  # Set of all the names of correlated columns\n",
    "        corr_matrix = data[i].corr()\n",
    "        for k in range(len(corr_matrix.columns)):\n",
    "            for j in range(k):\n",
    "                if abs(corr_matrix.iloc[k, j]) > threshold:\n",
    "                    colname = corr_matrix.columns[j]\n",
    "                    col_corr.add(colname)\n",
    "\n",
    "        data[i] = data[i].drop(data[i].columns[list(col_corr)], axis=1)\n",
    "\n",
    "remove_correlation(train_data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a54cf18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:01:41.290182Z",
     "iopub.status.busy": "2022-04-11T10:01:41.289352Z",
     "iopub.status.idle": "2022-04-11T10:01:41.292441Z",
     "shell.execute_reply": "2022-04-11T10:01:41.292933Z",
     "shell.execute_reply.started": "2022-04-11T09:41:49.380390Z"
    },
    "papermill": {
     "duration": 0.029948,
     "end_time": "2022-04-11T10:01:41.293130",
     "exception": false,
     "start_time": "2022-04-11T10:01:41.263182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 4, 3, 5], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [9, 1, 8, 6, 2, 4, 7, 5, 3], [1, 2, 3, 4, 5, 6, 7, 8, 9], [5, 6, 7, 4, 8, 3]]\n"
     ]
    }
   ],
   "source": [
    "# get labels for each dataset\n",
    "def get_unique_values(label):\n",
    "    unique_values = []\n",
    "    for i in range(len(label)):\n",
    "        unique_values.append(pd.unique(label[i][0].values).tolist())\n",
    "    return unique_values\n",
    "\n",
    "labels = get_unique_values(train_label)    \n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d230e499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:01:41.336768Z",
     "iopub.status.busy": "2022-04-11T10:01:41.336036Z",
     "iopub.status.idle": "2022-04-11T10:01:41.338368Z",
     "shell.execute_reply": "2022-04-11T10:01:41.338846Z",
     "shell.execute_reply.started": "2022-04-11T09:41:49.390775Z"
    },
    "papermill": {
     "duration": 0.026453,
     "end_time": "2022-04-11T10:01:41.339036",
     "exception": false,
     "start_time": "2022-04-11T10:01:41.312583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selected features for each dataset using selectkbest\n",
    "# la = [1,2,3,4,5,6,7,8,9]\n",
    "# def select_features(data, labels):\n",
    "#     threshold = 0.8\n",
    "#     selected_features = []\n",
    "#     for label in labels:\n",
    "#         selector = SelectKBest(chi2, k='all')\n",
    "#         selector.fit(data, label)\n",
    "#         selected_features.append(list(selector.scores_))\n",
    "#     selected_features = np.mean(selected_features, axis=0) > threshold\n",
    "#     print(selected_features)\n",
    "#     selected_features = np.max(selected_features, axis=0) > threshold\n",
    "#     print(selected_features)\n",
    "\n",
    "# select_features(train_data[2], la)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61f89f",
   "metadata": {
    "papermill": {
     "duration": 0.019048,
     "end_time": "2022-04-11T10:01:41.377349",
     "exception": false,
     "start_time": "2022-04-11T10:01:41.358301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46079a1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:01:41.425242Z",
     "iopub.status.busy": "2022-04-11T10:01:41.423847Z",
     "iopub.status.idle": "2022-04-11T10:01:41.440526Z",
     "shell.execute_reply": "2022-04-11T10:01:41.441034Z",
     "shell.execute_reply.started": "2022-04-11T09:41:49.404088Z"
    },
    "papermill": {
     "duration": 0.04477,
     "end_time": "2022-04-11T10:01:41.441217",
     "exception": false,
     "start_time": "2022-04-11T10:01:41.396447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset 1: 105\n",
      "Test dataset 1: 45\n",
      "Train dataset 2: 70\n",
      "Test dataset 2: 30\n",
      "Train dataset 3: 4410\n",
      "Test dataset 3: 1890\n",
      "Train dataset 4: 1782\n",
      "Test dataset 4: 765\n",
      "Train dataset 5: 783\n",
      "Test dataset 5: 336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Splitting the train data into training and testing\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "def split_data(data, label):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3, random_state = 21)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    X_train_i, X_test_i, y_train_i, y_test_i = split_data(train_data[i], train_label[i])\n",
    "    X_train.append(X_train_i)\n",
    "    X_test.append(X_test_i)\n",
    "    y_train.append(y_train_i)\n",
    "    y_test.append(y_test_i)\n",
    "    print(\"Train dataset \" + str(i+1) + \": \" + str(len(X_train_i)))\n",
    "    print(\"Test dataset \" + str(i+1) + \": \" + str(len(X_test_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0457846",
   "metadata": {
    "papermill": {
     "duration": 0.019333,
     "end_time": "2022-04-11T10:01:41.480327",
     "exception": false,
     "start_time": "2022-04-11T10:01:41.460994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83c9d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:01:41.526077Z",
     "iopub.status.busy": "2022-04-11T10:01:41.525378Z",
     "iopub.status.idle": "2022-04-11T10:01:41.528407Z",
     "shell.execute_reply": "2022-04-11T10:01:41.527812Z",
     "shell.execute_reply.started": "2022-04-11T09:41:49.428818Z"
    },
    "papermill": {
     "duration": 0.028373,
     "end_time": "2022-04-11T10:01:41.528548",
     "exception": false,
     "start_time": "2022-04-11T10:01:41.500175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, val=None):\n",
    "        self.feature_index = feature_index \n",
    "        self.threshold = threshold \n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.val = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c7b7ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:01:41.581677Z",
     "iopub.status.busy": "2022-04-11T10:01:41.580493Z",
     "iopub.status.idle": "2022-04-11T10:01:41.602548Z",
     "shell.execute_reply": "2022-04-11T10:01:41.601868Z",
     "shell.execute_reply.started": "2022-04-11T09:41:49.437422Z"
    },
    "papermill": {
     "duration": 0.05299,
     "end_time": "2022-04-11T10:01:41.602696",
     "exception": false,
     "start_time": "2022-04-11T10:01:41.549706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        ''' recursive function to build the tree ''' \n",
    "        \n",
    "        X, y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "\n",
    "        leaf_val = self.calculate_leaf_value(y)\n",
    "        return Node(val=leaf_val)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            for threshold in possible_thresholds:\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"gini\"):\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            # use entropy\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.val is not None:\n",
    "            print(tree.val)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        \n",
    "        if tree.val!=None: return tree.val\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97cbd6bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T10:01:41.655254Z",
     "iopub.status.busy": "2022-04-11T10:01:41.654533Z",
     "iopub.status.idle": "2022-04-11T10:14:23.531083Z",
     "shell.execute_reply": "2022-04-11T10:14:23.531660Z",
     "shell.execute_reply.started": "2022-04-11T09:41:49.471281Z"
    },
    "papermill": {
     "duration": 761.909609,
     "end_time": "2022-04-11T10:14:23.532068",
     "exception": false,
     "start_time": "2022-04-11T10:01:41.622459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for classifier 0: 0.8444444444444444\n",
      "Training Accuracy for classifier 0: 1.0\n",
      "Accuracy for classifier 1: 0.5\n",
      "Training Accuracy for classifier 1: 0.7857142857142857\n",
      "Accuracy for classifier 2: 0.3386243386243386\n",
      "Training Accuracy for classifier 2: 0.3653061224489796\n",
      "Accuracy for classifier 3: 0.5751633986928104\n",
      "Training Accuracy for classifier 3: 0.654320987654321\n",
      "Accuracy for classifier 4: 0.5416666666666666\n",
      "Training Accuracy for classifier 4: 0.6832694763729247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=5)\n",
    "\n",
    "\n",
    "classifier.fit(X_train[0], y_train[0])\n",
    "predictions = classifier.predict(X_test[0].values)\n",
    "train_predictions_zero = classifier.predict(X_train[0].values)\n",
    "print(\"Accuracy for classifier 0:\", accuracy_score(y_test[0], predictions))\n",
    "print(\"Training Accuracy for classifier 0:\", accuracy_score(y_train[0], train_predictions_zero))\n",
    "classifier.fit(X_train[1], y_train[1])\n",
    "predictions = classifier.predict(X_test[1].values)\n",
    "train_predictions_one = classifier.predict(X_train[1].values)\n",
    "print(\"Accuracy for classifier 1:\", accuracy_score(y_test[1], predictions))\n",
    "print(\"Training Accuracy for classifier 1:\", accuracy_score(y_train[1], train_predictions_one))\n",
    "classifier.fit(X_train[2], y_train[2])\n",
    "predictions = classifier.predict(X_test[2].values)\n",
    "train_predictions_two = classifier.predict(X_train[2].values)\n",
    "print(\"Accuracy for classifier 2:\", accuracy_score(y_test[2], predictions))\n",
    "print(\"Training Accuracy for classifier 2:\", accuracy_score(y_train[2], train_predictions_two))\n",
    "classifier.fit(X_train[3], y_train[3])\n",
    "predictions = classifier.predict(X_test[3].values)\n",
    "train_predictions_three = classifier.predict(X_train[3].values)\n",
    "print(\"Accuracy for classifier 3:\", accuracy_score(y_test[3], predictions))\n",
    "print(\"Training Accuracy for classifier 3:\", accuracy_score(y_train[3], train_predictions_three))\n",
    "classifier.fit(X_train[4], y_train[4])\n",
    "predictions = classifier.predict(X_test[4].values)\n",
    "train_predictions_four = classifier.predict(X_train[4].values)\n",
    "print(\"Accuracy for classifier 4:\", accuracy_score(y_test[4], predictions))\n",
    "print(\"Training Accuracy for classifier 4:\", accuracy_score(y_train[4], train_predictions_four))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658a2e4",
   "metadata": {
    "papermill": {
     "duration": 0.020928,
     "end_time": "2022-04-11T10:14:23.574849",
     "exception": false,
     "start_time": "2022-04-11T10:14:23.553921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 833.347235,
   "end_time": "2022-04-11T10:14:24.410339",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-11T10:00:31.063104",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
