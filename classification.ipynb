{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification: \n",
    "Classification is to identify which category a new observation belongs to, on the basis of a training dataset. There are five datasets. For each dataset, we provide the training dataset, training label, and test dataset. Please use the training dataset and training label to build your classifier and predict the test label. A class label is represented by an integer. For example, in the 1st dataset, there are 4 classes where 1 represents the 1st class, 2 represents the 2nd class, etc. Note that, there exist some missing values in some of the dataset (a missing entry is filled by 1.00000000000000e+99), please fill the missing values before perform your classification algorithm.\n",
    "\n",
    "TrainData 1 contains 3312 features with 150 samples. Testdata1 contains 3312 features with 53 samples. There are 5 classes in this dataset.\n",
    "\n",
    "TrainData 2 contains 9182 features with 100 samples. Testdata2 contains 9182 features with 74 samples. There are 11 classes in this dataset.\n",
    "\n",
    "TrainData 3 contains 13  features with 6300 samples. Testdata3 contains 13 features with 2693 samples. There are 9 classes in this dataset.\n",
    "\n",
    "TrainData 4 contains 112 features with 2547 samples. Testdata4 contains 112 features with 1092 samples. There are 9 classes in this dataset.\n",
    "\n",
    "TrainData 5 contains 11 features with 1119 samples. Testdata5 contains 11 features with 480 samples. There are 6 classes in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "traindata1 = pd.read_csv('input/TrainData1.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "traindata2 = pd.read_csv('input/TrainData2.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "traindata3 = pd.read_csv('input/TrainData3.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "traindata4 = pd.read_csv('input/TrainData4.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "traindata5 = pd.read_csv('input/TrainData5.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "train_data = [traindata1, traindata2, traindata3, traindata4, traindata5]\n",
    "\n",
    "testdata1 = pd.read_csv('input/TestData1.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "testdata2 = pd.read_csv('input/TestData2.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "testdata3 = pd.read_csv('input/TestData3.txt', delimiter = \",\", header=None, na_values=1000000000)\n",
    "testdata4 = pd.read_csv('input/TestData4.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "testdata5 = pd.read_csv('input/TestData5.txt', sep='\\s+', header=None, na_values='1.00000000000000e+99')\n",
    "test_data = [testdata1, testdata2, testdata3, testdata4, testdata5]\n",
    "\n",
    "trainlabel1 = pd.read_csv('input/TrainLabel1.txt', sep='\\t', header=None)\n",
    "trainlabel2 = pd.read_csv('input/TrainLabel2.txt', sep='\\t', header=None)\n",
    "trainlabel3 = pd.read_csv('input/TrainLabel3.txt', sep='\\t', header=None)\n",
    "trainlabel4 = pd.read_csv('input/TrainLabel4.txt', sep='\\t', header=None)\n",
    "trainlabel5 = pd.read_csv('input/TrainLabel5.txt', sep='\\t', header=None)\n",
    "trainlabel1 = trainlabel1.values.ravel()\n",
    "trainlabel2 = trainlabel2.values.ravel()\n",
    "trainlabel3 = trainlabel3.values.ravel()\n",
    "trainlabel4 = trainlabel4.values.ravel()\n",
    "trainlabel5 = trainlabel5.values.ravel()\n",
    "train_label = [trainlabel1, trainlabel2, trainlabel3, trainlabel4, trainlabel5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the train data:\n",
      "train dataset 1: 9936\n",
      "train dataset 2: 0\n",
      "train dataset 3: 1886\n",
      "train dataset 4: 0\n",
      "train dataset 5: 0\n",
      "Missing values in the test data:\n",
      "test dataset 1: 7021\n",
      "test dataset 2: 0\n",
      "test dataset 3: 808\n",
      "test dataset 4: 0\n",
      "test dataset 5: 0\n"
     ]
    }
   ],
   "source": [
    "# count the numbers of coloumns where the value = 1.00000000000000e+99\n",
    "def count_na(data, type):\n",
    "    print(\"Missing values in the \" + type + \" data:\")\n",
    "    for i in range(len(data)):\n",
    "        missing_val_count = data[i].isnull().sum().sum()\n",
    "        print( type + \" dataset \" + str(i+1) + \": \" + str(missing_val_count))\n",
    "\n",
    "count_na(train_data, \"train\")\n",
    "count_na(test_data, \"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    k = int(np.sqrt(train_data[i].shape[0])/2)  # k = square root of the number of rows\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    train_data[i] = imputer.fit_transform(train_data[i])\n",
    "    test_data[i] = imputer.transform(test_data[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling / undersamping\n",
    "# dataset 1:\n",
    "oversample_1 = RandomOverSampler(sampling_strategy={1: 108, 2: 60, 4: 60, 3: 55, 5: 50})\n",
    "train_data[0], train_label[0] = oversample_1.fit_resample(train_data[0], train_label[0])\n",
    "\n",
    "# dataset 3:\n",
    "oversample_3 = RandomOverSampler(sampling_strategy={1: 1235, 8: 1200, 6: 1200, 7: 1200, 9: 1200, 4: 1200, 2: 1200, 5: 1200, 3: 1200})\n",
    "train_data[2], train_label[2] = oversample_3.fit_resample(train_data[2], train_label[2])\n",
    "\n",
    "#dataset 5:\n",
    "oversample_5 = RandomOverSampler(sampling_strategy={5: 471, 6: 447, 7: 400, 4: 300, 8: 200, 3: 200})\n",
    "train_data[4], train_label[4] = oversample_5.fit_resample(train_data[4], train_label[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset 1: 233\n",
      "Test dataset 1: 100\n",
      "Train dataset 2: 70\n",
      "Test dataset 2: 30\n",
      "Train dataset 3: 7584\n",
      "Test dataset 3: 3251\n",
      "Train dataset 4: 1782\n",
      "Test dataset 4: 765\n",
      "Train dataset 5: 1412\n",
      "Test dataset 5: 606\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing data\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "def split_data(data, label):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3, random_state = 2)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    X_train_i, X_test_i, y_train_i, y_test_i = split_data(train_data[i], train_label[i])\n",
    "    X_train.append(X_train_i)\n",
    "    X_test.append(X_test_i)\n",
    "    y_train.append(y_train_i)\n",
    "    y_test.append(y_test_i)\n",
    "    print(\"Train dataset \" + str(i+1) + \": \" + str(len(X_train_i)))\n",
    "    print(\"Test dataset \" + str(i+1) + \": \" + str(len(X_test_i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "for i in range(len(train_data)):\n",
    "    X_train[i] = sc.fit_transform(X_train[i])\n",
    "    X_test[i] = sc.transform(X_test[i])\n",
    "    test_data[i] = sc.transform(test_data[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: 0.99\n",
      "Dataset 2: 0.9\n",
      "Dataset 3: 0.6539526299600124\n",
      "Dataset 4: 0.9124183006535947\n",
      "Dataset 5: 0.8151815181518152\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "test_labels = []\n",
    "# dataset 1:\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50, 32),activation=\"relu\",random_state=1,max_iter=2000).fit(X_train[0], y_train[0])\n",
    "train_pred = clf.predict(X_test[0])\n",
    "print(\"Dataset 1: \" + str(accuracy_score(y_test[0], train_pred)))\n",
    "test_labels.append(clf.predict(test_data[0]))\n",
    "\n",
    "# dataset 2:\n",
    "clf = MLPClassifier(hidden_layer_sizes=(75,64,32),activation=\"relu\",random_state=1,max_iter=2000).fit(X_train[1], y_train[1])\n",
    "train_pred = clf.predict(X_test[1])\n",
    "print(\"Dataset 2: \" + str(accuracy_score(y_test[1], train_pred)))\n",
    "test_labels.append(clf.predict(test_data[1]))\n",
    "\n",
    "# dataset 3:\n",
    "clf = MLPClassifier(hidden_layer_sizes=(500,300,150,75,32),activation=\"relu\",random_state=1,max_iter=3000).fit(X_train[2], y_train[2])\n",
    "train_pred = clf.predict(X_test[2])\n",
    "print(\"Dataset 3: \" + str(accuracy_score(y_test[2], train_pred)))\n",
    "test_labels.append(clf.predict(test_data[2]))\n",
    "\n",
    "# dataset 4:\n",
    "clf = MLPClassifier(hidden_layer_sizes=(500,150,75,32),activation=\"relu\",random_state=1,max_iter=3000).fit(X_train[3], y_train[3])\n",
    "train_pred = clf.predict(X_test[3])\n",
    "print(\"Dataset 4: \" + str(accuracy_score(y_test[3], train_pred)))\n",
    "test_labels.append(clf.predict(test_data[3]))\n",
    "\n",
    "# dataset 5:\n",
    "clf = MLPClassifier(hidden_layer_sizes=(500,128,64,32),activation=\"relu\",random_state=1,max_iter=3000).fit(X_train[4], y_train[4])\n",
    "train_pred = clf.predict(X_test[4])\n",
    "print(\"Dataset 5: \" + str(accuracy_score(y_test[4], train_pred)))\n",
    "test_labels.append(clf.predict(test_data[4]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the test labels to file\n",
    "for i in range(len(test_labels)):\n",
    "    test_labels[i] = test_labels[i].astype(int)\n",
    "    test_labels[i] = pd.DataFrame(test_labels[i])\n",
    "    test_labels[i].to_csv('output/MohamedClassification'+str(i+1)+'.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to replace the missing values in the dataset using the mean of each column\n",
    "# def replace_na(data):\n",
    "#     for i in range(len(data)):\n",
    "#         for column in data[i].columns:   \n",
    "#                 data[i][column] = data[i][column].fillna(data[i][column].mean())\n",
    "         \n",
    "# replace_na(train_data)\n",
    "\n",
    "# # Replacing test dataset 1 missing values\n",
    "# for column in testdata1.columns:\n",
    "#     testdata1[column] = testdata1[column].fillna(testdata1[column].mean())\n",
    "\n",
    "# # Now checking if any remaining missing vals left\n",
    "# count_na(train_data, \"train\")\n",
    "# count_na(test_data, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the train data, remove the columns with low variance\n",
    "def remove_low_variance(data):\n",
    "    for i in range(len(data)):\n",
    "        selector = VarianceThreshold(threshold=.8 * (1 - .8))\n",
    "        data[i] = selector.fit_transform(data[i])\n",
    "\n",
    "remove_low_variance(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated features\n",
    "def remove_correlation(data, threshold):\n",
    "    for i in range(len(data)):\n",
    "        data[i] = pd.DataFrame(data[i])\n",
    "        col_corr = set()  # Set of all the names of correlated columns\n",
    "        corr_matrix = data[i].corr()\n",
    "        for k in range(len(corr_matrix.columns)):\n",
    "            for j in range(k):\n",
    "                if abs(corr_matrix.iloc[k, j]) > threshold:\n",
    "                    colname = corr_matrix.columns[j]\n",
    "                    col_corr.add(colname)\n",
    "\n",
    "        data[i] = data[i].drop(data[i].columns[list(col_corr)], axis=1)\n",
    "\n",
    "remove_correlation(train_data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-d25b9aff0f1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0munique_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_unique_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-d25b9aff0f1c>\u001b[0m in \u001b[0;36mget_unique_values\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0munique_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0munique_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0munique_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# get labels for each dataset\n",
    "def get_unique_values(label):\n",
    "    unique_values = []\n",
    "    for i in range(len(label)):\n",
    "        unique_values.append(pd.unique(label[i][0].values).tolist())\n",
    "    return unique_values\n",
    "\n",
    "labels = get_unique_values(train_label)    \n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected features for each dataset using selectkbest\n",
    "# la = [1,2,3,4,5,6,7,8,9]\n",
    "# def select_features(data, labels):\n",
    "#     threshold = 0.8\n",
    "#     selected_features = []\n",
    "#     for label in labels:\n",
    "#         selector = SelectKBest(chi2, k='all')\n",
    "#         selector.fit(data, label)\n",
    "#         selected_features.append(list(selector.scores_))\n",
    "#     selected_features = np.mean(selected_features, axis=0) > threshold\n",
    "#     print(selected_features)\n",
    "#     selected_features = np.max(selected_features, axis=0) > threshold\n",
    "#     print(selected_features)\n",
    "\n",
    "# select_features(train_data[2], la)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Splitting the train data into training and testing\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "def split_data(data, label):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3, random_state = 21)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    X_train_i, X_test_i, y_train_i, y_test_i = split_data(train_data[i], train_label[i])\n",
    "    X_train.append(X_train_i)\n",
    "    X_test.append(X_test_i)\n",
    "    y_train.append(y_train_i)\n",
    "    y_test.append(y_test_i)\n",
    "    print(\"Train dataset \" + str(i+1) + \": \" + str(len(X_train_i)))\n",
    "    print(\"Test dataset \" + str(i+1) + \": \" + str(len(X_test_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are creating are decision tree classifier https://www.youtube.com/watch?v=sgQAhG5Q7iY\n",
    "# First the node class - describes node of the tree\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        # decision defined by feature index and threshold for that feature\n",
    "        self.feature_index = feature_index \n",
    "        self.threshold = threshold \n",
    "        # acces left and right child nodes \n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        # information gain of splitting data\n",
    "        self.info_gain = info_gain\n",
    "        # leaf node\n",
    "        self.value = value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        ''' recursive function to build the tree ''' \n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"gini\"):\n",
    "        ''' function to compute information gain '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        ''' function to compute entropy '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        ''' function to compute gini index '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=5)\n",
    "\n",
    "\n",
    "classifier.fit(X_train[0], y_train[0])\n",
    "predictions = classifier.predict(X_test[0].values)\n",
    "print(\"Accuracy for classifier 0:\", accuracy_score(y_test[0], predictions))\n",
    "\n",
    "classifier.fit(X_train[1], y_train[1])\n",
    "predictions = classifier.predict(X_test[1].values)\n",
    "print(\"Accuracy for classifier 1:\", accuracy_score(y_test[1], predictions))\n",
    "\n",
    "classifier.fit(X_train[2], y_train[2])\n",
    "predictions = classifier.predict(X_test[2].values)\n",
    "print(\"Accuracy for classifier 2:\", accuracy_score(y_test[2], predictions))\n",
    "\n",
    "classifier.fit(X_train[3], y_train[3])\n",
    "predictions = classifier.predict(X_test[3].values)\n",
    "print(\"Accuracy for classifier 3:\", accuracy_score(y_test[3], predictions))\n",
    "\n",
    "classifier.fit(X_train[4], y_train[4])\n",
    "predictions = classifier.predict(X_test[4].values)\n",
    "print(\"Accuracy for classifier 4:\", accuracy_score(y_test[4], predictions))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea458d1793ed83c4a9a1c9829b22cc5321347abf427211848a85b4a95e9b09b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
